{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c94d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def hash(coords, log2_hashmap_size):\n",
    "    '''\n",
    "    coords: this function can process upto 7 dim coordinates\n",
    "    log2T:  logarithm of T w.r.t 2\n",
    "    '''\n",
    "    primes = [1, 2654435761, 805459861, 3674653429, 2097192037, 1434869437, 2165219737]\n",
    "\n",
    "    xor_result = torch.zeros_like(coords)[..., 0] # 任何数与0做位异或操作结果都是这个数本身!\n",
    "    for i in range(coords.shape[-1]):\n",
    "        xor_result ^= coords[..., i]*primes[i]    # python中^是按位异或运算符，运算规则是：将两个数的二进制按位对比，如果相同（都为1或者都为0），则运算结果的对应位上是0，否则为1\n",
    "\n",
    "    return torch.tensor((1<<log2_hashmap_size)-1).to(xor_result.device) & xor_result # 1 << log2_hashmap_size代表将1的二进制左移log2_hashmap_size位，然后在其右边全补上0，将结果换算为十进制等于2的log2_hashmap_size次方\n",
    "                                                                                     # 2的N次方-1在二进制中是一个十分特殊的数，它右N个1从最右向左依次排列组成\n",
    "                                                                                     \n",
    "                                                                                     # python中&是位与运算符，运算规则是：将两个数的二进制按位对比，都为1，则结果的对应位上是1，否则为0\n",
    "                                                                                     # 对于一个数x（十进制），x mod 2的N次方 等价于取x的二进制的后N位数（得到的是二进制，需要将其转化为十进制）\n",
    "                                                                                     # 例如，对109（十进制），x mod 2的4次方(16) = 13:\n",
    "                                                                                     # 109的二进制为1101101\n",
    "                                                                                     #  13的二进制为   1101\n",
    "                                                                                     # 因此这行代码等效于 xor_result mod 2的log2_hashmap_size\n",
    "\n",
    "def get_voxel_vertices(xyz, bounding_box, resolution, log2_hashmap_size):\n",
    "    '''\n",
    "    xyz: 3D coordinates of samples. B x 3\n",
    "    bounding_box: min and max x,y,z coordinates of object bbox\n",
    "    resolution: number of voxels per axis\n",
    "    '''\n",
    "    box_min, box_max = bounding_box\n",
    "    BOX_OFFSETS = torch.tensor([[[i,j,k] for i in [0, 1] for j in [0, 1] for k in [0, 1]]]).to(xyz.device)\n",
    "    \n",
    "    keep_mask = xyz==torch.max(torch.min(xyz, box_max), box_min)\n",
    "    if not torch.all(xyz <= box_max) or not torch.all(xyz >= box_min):\n",
    "        # print(\"ALERT: some points are outside bounding box. Clipping them!\")\n",
    "        xyz = torch.clamp(xyz, min=box_min, max=box_max)\n",
    "\n",
    "    grid_size = (box_max-box_min)/resolution\n",
    "    \n",
    "    bottom_left_idx = torch.floor((xyz-box_min)/grid_size).int() # bottom_left_idx.shape = [B, 3]\n",
    "    voxel_min_vertex = bottom_left_idx*grid_size + box_min\n",
    "    voxel_max_vertex = voxel_min_vertex + torch.tensor([1.0,1.0,1.0]).to(xyz.device)*grid_size\n",
    "\n",
    "    voxel_indices = bottom_left_idx.unsqueeze(1) + BOX_OFFSETS # shape: [B, 1, 3] +　[1, 8, 3]\n",
    "    hashed_voxel_indices = hash(voxel_indices, log2_hashmap_size)\n",
    "\n",
    "    return voxel_min_vertex, voxel_max_vertex, hashed_voxel_indices, keep_mask\n",
    "\n",
    "class HashEmbedder(nn.Module):\n",
    "    def __init__(self, bounding_box, n_levels=16, n_features_per_level=2,\\\n",
    "                log2_hashmap_size=19, base_resolution=16, finest_resolution=512):\n",
    "        super(HashEmbedder, self).__init__()\n",
    "        self.bounding_box = bounding_box\n",
    "        self.n_levels = n_levels                                  # 多少个级别的hashmap，对应论文中的L\n",
    "        self.n_features_per_level = n_features_per_level          # hashmap中的每个特征的维度，对应论文中的F\n",
    "        self.log2_hashmap_size = log2_hashmap_size                # 每个hashmap中有多少个特征（2的指数次个），对应论文中的T\n",
    "        self.base_resolution = torch.tensor(base_resolution)\n",
    "        self.finest_resolution = torch.tensor(finest_resolution)\n",
    "        self.out_dim = self.n_levels * self.n_features_per_level\n",
    "\n",
    "        self.b = torch.exp((torch.log(self.finest_resolution)-torch.log(self.base_resolution))/(n_levels-1))\n",
    "\n",
    "        # nn.Embedding(num_embeddings, embedding_dim)创建了一个Lookup table, 用来实现hashmap\n",
    "        # Parameters:\n",
    "        # num_beddings: 特征的数量 embedding_dim：每个特征的维度\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(2**self.log2_hashmap_size, \\\n",
    "                                        self.n_features_per_level) for i in range(n_levels)])\n",
    "        # custom uniform initialization\n",
    "        for i in range(n_levels):\n",
    "            nn.init.uniform_(self.embeddings[i].weight, a=-0.0001, b=0.0001)\n",
    "            # self.embeddings[i].weight.data.zero_()\n",
    "        \n",
    "\n",
    "    def trilinear_interp(self, x, voxel_min_vertex, voxel_max_vertex, voxel_embedds):\n",
    "        '''\n",
    "        x: B x 3\n",
    "        voxel_min_vertex: B x 3\n",
    "        voxel_max_vertex: B x 3\n",
    "        voxel_embedds: B x 8 x 2\n",
    "        '''\n",
    "        # source: https://en.wikipedia.org/wiki/Trilinear_interpolation\n",
    "        weights = (x - voxel_min_vertex)/(voxel_max_vertex-voxel_min_vertex) # B x 3\n",
    "\n",
    "        # step 1\n",
    "        # 0->000, 1->001, 2->010, 3->011, 4->100, 5->101, 6->110, 7->111\n",
    "        c00 = voxel_embedds[:,0]*(1-weights[:,0][:,None]) + voxel_embedds[:,4]*weights[:,0][:,None]\n",
    "        c01 = voxel_embedds[:,1]*(1-weights[:,0][:,None]) + voxel_embedds[:,5]*weights[:,0][:,None]\n",
    "        c10 = voxel_embedds[:,2]*(1-weights[:,0][:,None]) + voxel_embedds[:,6]*weights[:,0][:,None]\n",
    "        c11 = voxel_embedds[:,3]*(1-weights[:,0][:,None]) + voxel_embedds[:,7]*weights[:,0][:,None]\n",
    "\n",
    "        # step 2\n",
    "        c0 = c00*(1-weights[:,1][:,None]) + c10*weights[:,1][:,None]\n",
    "        c1 = c01*(1-weights[:,1][:,None]) + c11*weights[:,1][:,None]\n",
    "\n",
    "        # step 3\n",
    "        c = c0*(1-weights[:,2][:,None]) + c1*weights[:,2][:,None]\n",
    "\n",
    "        return c\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is 3D point position: B x 3\n",
    "        x_embedded_all = []\n",
    "        for i in range(self.n_levels):\n",
    "            resolution = torch.floor(self.base_resolution * self.b**i)\n",
    "            voxel_min_vertex, voxel_max_vertex, hashed_voxel_indices, keep_mask = get_voxel_vertices(\\\n",
    "                                                x, self.bounding_box, \\\n",
    "                                                resolution, self.log2_hashmap_size)\n",
    "            \n",
    "            voxel_embedds = self.embeddings[i](hashed_voxel_indices)\n",
    "\n",
    "            x_embedded = self.trilinear_interp(x, voxel_min_vertex, voxel_max_vertex, voxel_embedds)\n",
    "            x_embedded_all.append(x_embedded)\n",
    "\n",
    "        keep_mask = keep_mask.sum(dim=-1)==keep_mask.shape[-1]\n",
    "        return torch.cat(x_embedded_all, dim=-1), keep_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eaeb05",
   "metadata": {},
   "source": [
    "1. 设置测试函数，生成训练集与测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6446f152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cuda] on!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 602, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/zhx.zhang/miniconda3/envs/HashEmbedder/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_553011/2464121989.py\", line 16, in <module>\n",
      "    bounding_box = torch.tensor([[-5, -5, -5], [5, 5, 5]]).to(Device)\n",
      "/tmp/ipykernel_553011/2464121989.py:16: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /opt/conda/conda-bld/pytorch_1682343967769/work/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  bounding_box = torch.tensor([[-5, -5, -5], [5, 5, 5]]).to(Device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "Device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'[{Device}] on!')\n",
    "\n",
    "def TestFunction(x):\n",
    "    '''\n",
    "    x is 3D points position B x 3\n",
    "    '''\n",
    "    # result = torch.sin(torch.pi*x[..., 0]) + torch.cos(torch.pi*x[..., 1]) + torch.sin(2*torch.pi*x[..., 2])\n",
    "    # result = torch.sqrt(x[..., 0]**2 + x[..., 0]**2 + x[..., 2]**2)\n",
    "    result = x[..., 0 ] + x[..., 1]+ x[..., 2]\n",
    "    return result\n",
    "\n",
    "bounding_box = torch.tensor([[-5, -5, -5], [5, 5, 5]]).to(Device)\n",
    "\n",
    "def get_dataset(bouding_box, batch_size = 100, noise_std = 0.01):\n",
    "    coords_min = bouding_box[0]\n",
    "    coords_max = bouding_box[1]\n",
    "    length = coords_max - coords_min\n",
    "    weight = torch.rand([batch_size, 3]).to(bounding_box.device)\n",
    "    coords = coords_min + length * weight\n",
    "    \n",
    "    real_value = TestFunction(coords)\n",
    "    noise = torch.randn(real_value.shape, device = Device) * noise_std\n",
    "    value_withNoisy = real_value + noise\n",
    "    \n",
    "    return coords, value_withNoisy\n",
    "\n",
    "trainDataset_inputs, trainDataset_outputs = get_dataset(bounding_box, batch_size = 1000)\n",
    "testDataset_inputs, testDataset_outputs = get_dataset(bounding_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc09aec",
   "metadata": {},
   "source": [
    "2. 神经网络构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba208f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Test Loss: 24.1455\n",
      "Epoch [4/100], Test Loss: 23.8626\n",
      "Epoch [6/100], Test Loss: 18.1111\n",
      "Epoch [8/100], Test Loss: 17.1159\n",
      "Epoch [10/100], Test Loss: 16.8647\n",
      "Epoch [12/100], Test Loss: 16.7904\n",
      "Epoch [14/100], Test Loss: 16.6803\n",
      "Epoch [16/100], Test Loss: 16.7530\n",
      "Epoch [18/100], Test Loss: 16.7435\n",
      "Epoch [20/100], Test Loss: 16.7291\n",
      "Epoch [22/100], Test Loss: 16.7376\n",
      "Epoch [24/100], Test Loss: 16.7667\n",
      "Epoch [26/100], Test Loss: 16.7379\n",
      "Epoch [28/100], Test Loss: 16.7504\n",
      "Epoch [30/100], Test Loss: 16.7536\n",
      "Epoch [32/100], Test Loss: 16.7570\n",
      "Epoch [34/100], Test Loss: 16.7407\n",
      "Epoch [36/100], Test Loss: 16.7588\n",
      "Epoch [38/100], Test Loss: 16.7313\n",
      "Epoch [40/100], Test Loss: 16.7802\n",
      "Epoch [42/100], Test Loss: 16.7608\n",
      "Epoch [44/100], Test Loss: 16.7577\n",
      "Epoch [46/100], Test Loss: 16.7208\n",
      "Epoch [48/100], Test Loss: 16.7476\n",
      "Epoch [50/100], Test Loss: 16.7601\n",
      "Epoch [52/100], Test Loss: 16.7148\n",
      "Epoch [54/100], Test Loss: 16.7636\n",
      "Epoch [56/100], Test Loss: 16.7253\n",
      "Epoch [58/100], Test Loss: 16.7766\n",
      "Epoch [60/100], Test Loss: 16.6756\n",
      "Epoch [62/100], Test Loss: 16.6054\n",
      "Epoch [64/100], Test Loss: 16.5011\n",
      "Epoch [66/100], Test Loss: 16.5963\n",
      "Epoch [68/100], Test Loss: 16.5610\n",
      "Epoch [70/100], Test Loss: 16.4856\n",
      "Epoch [72/100], Test Loss: 16.3511\n",
      "Epoch [74/100], Test Loss: 16.4504\n",
      "Epoch [76/100], Test Loss: 16.3922\n",
      "Epoch [78/100], Test Loss: 16.2353\n",
      "Epoch [80/100], Test Loss: 16.2914\n",
      "Epoch [82/100], Test Loss: 16.2467\n",
      "Epoch [84/100], Test Loss: 16.1471\n",
      "Epoch [86/100], Test Loss: 16.2317\n",
      "Epoch [88/100], Test Loss: 16.2149\n",
      "Epoch [90/100], Test Loss: 16.1430\n",
      "Epoch [92/100], Test Loss: 16.0922\n",
      "Epoch [94/100], Test Loss: 16.0763\n",
      "Epoch [96/100], Test Loss: 16.0645\n",
      "Epoch [98/100], Test Loss: 16.0474\n",
      "Epoch [100/100], Test Loss: 16.0272\n",
      "\n",
      "Training finished!\n",
      "Training Time: 59.45481562614441 s\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim = 256, output_dim = 1):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope = 0.01)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope = 0.01)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu3 = nn.LeakyReLU(negative_slope = 0.01)\n",
    "        self.fc4 = nn.Linear(hidden_dim,  output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t = x\n",
    "        t = self.relu1(self.fc1(t))\n",
    "        t = self.relu2(self.fc2(t))\n",
    "        t = self.relu3(self.fc3(t))\n",
    "        result = self.fc4(t)\n",
    "        return result\n",
    "        \n",
    "MLPmodel = SimpleMLP(input_dim = 32).to(Device)\n",
    "hashembedder = HashEmbedder(bounding_box).to(Device)\n",
    "mini_batchSize = 100\n",
    "train_loader = DataLoader(\n",
    "    dataset = torch.utils.data.TensorDataset(trainDataset_inputs, trainDataset_outputs),\n",
    "    batch_size = mini_batchSize,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(list(MLPmodel.parameters()) + list(hashembedder.parameters()), lr = 0.001)\n",
    "mse = nn.MSELoss()\n",
    "num_epochs = 100\n",
    "trainingTime_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    MLPmodel.train()\n",
    "    hashembedder.train()\n",
    "    for miniBatch_trainingInputs, miniBatch_trainingOutputs in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        miniBatch_trainingEncodingInputs, _ = hashembedder(miniBatch_trainingInputs)\n",
    "        predictedValue = MLPmodel(miniBatch_trainingEncodingInputs)\n",
    "        loss = mse(miniBatch_trainingOutputs.unsqueeze(-1), predictedValue)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        MLPmodel.eval()\n",
    "        hashembedder.eval()\n",
    "        with torch.no_grad():\n",
    "            testDataset_encodingInputs, _ = hashembedder(testDataset_inputs)\n",
    "            predictedValue_testDataset = MLPmodel(testDataset_encodingInputs)\n",
    "            test_loss = mse(testDataset_outputs.unsqueeze(-1), predictedValue_testDataset)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Test Loss: {test_loss.item():.4f}')\n",
    "trainingTime_end = time.time()\n",
    "print('\\nTraining finished!')\n",
    "print(f'Training Time: {trainingTime_end - trainingTime_start} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b2332cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting visualization...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     embedded_grid_points, _ \u001b[38;5;241m=\u001b[39m hashembedder(grid_points)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# 预测函数值\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     predicted_values \u001b[38;5;241m=\u001b[39m \u001b[43mMLPmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded_grid_points\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 将预测值重塑回 2D 网格形式\u001b[39;00m\n\u001b[1;32m     30\u001b[0m predicted_values_grid \u001b[38;5;241m=\u001b[39m predicted_values\u001b[38;5;241m.\u001b[39mreshape(resolution_vis, resolution_vis)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "print(\"\\nStarting visualization...\")\n",
    "# 设置可视化网格的参数\n",
    "resolution_vis = 50 # 网格点的数量，例如 50x50\n",
    "# 确保 bounding_box 已经定义并移动到正确的设备上\n",
    "# 假设 bounding_box = torch.tensor([[-5, -5, -5], [5, 5, 5]]).to(Device) 已经在前面定义\n",
    "x_coords = torch.linspace(bounding_box[0,0].item(), bounding_box[1,0].item(), resolution_vis)\n",
    "y_coords = torch.linspace(bounding_box[0,1].item(), bounding_box[1,1].item(), resolution_vis)\n",
    "# 创建一个 2D 网格，并固定 z 为 0 （或边界内的任何常数）\n",
    "# 例如，我们固定 z 为 bounding_box 的中心点 z 坐标\n",
    "fixed_z = (bounding_box[0,2] + bounding_box[1,2]) / 2\n",
    "# 使用 meshgrid 创建 x-y 平面上的坐标\n",
    "# xx: resolution_vis x resolution_vis (x 值重复)\n",
    "# yy: resolution_vis x resolution_vis (y 值重复)\n",
    "xx, yy = torch.meshgrid(x_coords, y_coords, indexing='ij') # 使用 'ij' 保持矩阵索引习惯\n",
    "# 将网格展平并组合成 B x 3 的形式\n",
    "# B = resolution_vis * resolution_vis\n",
    "grid_points = torch.stack([xx.flatten(), yy.flatten(), torch.full_like(xx.flatten(), fixed_z)], dim=-1).to(Device)\n",
    "# 将模型设置为评估模式\n",
    "MLPmodel.eval()\n",
    "hashembedder.eval()\n",
    "# 使用 torch.no_grad() 禁用梯度计算，节省内存并加速\n",
    "with torch.no_grad():\n",
    "    # 编码网格点\n",
    "    embedded_grid_points, _ = hashembedder(grid_points)\n",
    "    # 预测函数值\n",
    "    predicted_values = MLPmodel(embedded_grid_points).cpu().numpy()\n",
    "# 将预测值重塑回 2D 网格形式\n",
    "predicted_values_grid = predicted_values.reshape(resolution_vis, resolution_vis)\n",
    "# 计算真实值进行对比\n",
    "true_values = TestFunction(grid_points).cpu().numpy().reshape(resolution_vis, resolution_vis)\n",
    "# 绘制结果\n",
    "plt.figure(figsize=(14, 6))\n",
    "# 绘制真实函数值\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(true_values, origin='lower', extent=[x_coords.min(), x_coords.max(), y_coords.min(), y_coords.max()], cmap='viridis')\n",
    "plt.colorbar(label='True Value')\n",
    "plt.title(f'True Function Value (Z={fixed_z.item():.2f})') # 使用 .item() 获取标量值\n",
    "plt.xlabel('X-coordinate')\n",
    "plt.ylabel('Y-coordinate')\n",
    "# 绘制模型预测值\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(predicted_values_grid, origin='lower', extent=[x_coords.min(), x_coords.max(), y_coords.min(), y_coords.max()], cmap='viridis')\n",
    "plt.colorbar(label='Predicted Value')\n",
    "plt.title(f'Predicted Function Value (Z={fixed_z.item():.2f})')\n",
    "plt.xlabel('X-coordinate')\n",
    "plt.ylabel('Y-coordinate')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 绘制误差图\n",
    "plt.figure(figsize=(7, 6))\n",
    "error_map = np.abs(true_values - predicted_values_grid)\n",
    "plt.imshow(error_map, origin='lower', extent=[x_coords.min(), x_coords.max(), y_coords.min(), y_coords.max()], cmap='Reds')\n",
    "plt.colorbar(label='Absolute Error')\n",
    "plt.title(f'Absolute Error Map (Z={fixed_z.item():.2f})')\n",
    "plt.xlabel('X-coordinate')\n",
    "plt.ylabel('Y-coordinate')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HashEncode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
